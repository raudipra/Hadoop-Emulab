17/08/17 22:02:53 INFO client.RMProxy: Connecting to ResourceManager at node-1/10.1.1.3:8032
Max number of map tasks 150
Max number of red tasks 30
shuffleInputRatio  = 4.3845975E-7
outputShuffleRatio = 1.0
Running on 15 nodes with 135 maps and 1 reduces.
0.20375046324501267
0.6463009382685494
Job started: Thu Aug 17 22:02:54 MDT 2017
17/08/17 22:02:54 INFO client.RMProxy: Connecting to ResourceManager at node-1/10.1.1.3:8032
17/08/17 22:02:54 INFO client.RMProxy: Connecting to ResourceManager at node-1/10.1.1.3:8032
17/08/17 22:02:54 INFO mapred.FileInputFormat: Total input paths to process : 41
17/08/17 22:02:55 INFO mapreduce.JobSubmitter: number of splits:82
17/08/17 22:02:55 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1503028541463_0064
17/08/17 22:02:55 INFO impl.YarnClientImpl: Submitted application application_1503028541463_0064
17/08/17 22:02:55 INFO mapreduce.Job: The url to track the job: http://node-1:8088/proxy/application_1503028541463_0064/
17/08/17 22:02:55 INFO mapreduce.Job: Running job: job_1503028541463_0064
17/08/17 22:09:11 INFO mapreduce.Job: Job job_1503028541463_0064 running in uber mode : false
17/08/17 22:09:11 INFO mapreduce.Job:  map 0% reduce 0%
17/08/17 22:09:18 INFO mapreduce.Job:  map 27% reduce 0%
17/08/17 22:09:19 INFO mapreduce.Job:  map 48% reduce 0%
17/08/17 22:09:20 INFO mapreduce.Job:  map 49% reduce 0%
17/08/17 22:09:21 INFO mapreduce.Job:  map 74% reduce 0%
17/08/17 22:09:22 INFO mapreduce.Job:  map 91% reduce 0%
17/08/17 22:09:23 INFO mapreduce.Job:  map 94% reduce 0%
17/08/17 22:09:28 INFO mapreduce.Job:  map 96% reduce 0%
17/08/17 22:09:30 INFO mapreduce.Job:  map 97% reduce 31%
17/08/17 22:09:31 INFO mapreduce.Job:  map 98% reduce 31%
17/08/17 22:09:33 INFO mapreduce.Job:  map 98% reduce 33%
17/08/17 22:09:43 INFO mapreduce.Job:  map 99% reduce 33%
17/08/17 22:09:58 INFO mapreduce.Job:  map 100% reduce 33%
17/08/17 22:10:00 INFO mapreduce.Job:  map 100% reduce 100%
17/08/17 22:10:00 INFO mapreduce.Job: Job job_1503028541463_0064 completed successfully
17/08/17 22:10:00 INFO mapreduce.Job: Counters: 55
	File System Counters
		FILE: Number of bytes read=886
		FILE: Number of bytes written=9954058
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=3222350228
		HDFS: Number of bytes written=1024
		HDFS: Number of read operations=331
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Killed map tasks=2
		Launched map tasks=84
		Launched reduce tasks=1
		Data-local map tasks=72
		Rack-local map tasks=12
		Total time spent by all maps in occupied slots (ms)=681140
		Total time spent by all reduces in occupied slots (ms)=39404
		Total time spent by all map tasks (ms)=681140
		Total time spent by all reduce tasks (ms)=39404
		Total vcore-seconds taken by all map tasks=681140
		Total vcore-seconds taken by all reduce tasks=39404
		Total megabyte-seconds taken by all map tasks=697487360
		Total megabyte-seconds taken by all reduce tasks=40349696
	Map-Reduce Framework
		Map input records=27514649
		Map output records=8
		Map output bytes=864
		Map output materialized bytes=1372
		Input split bytes=8856
		Combine input records=0
		Combine output records=0
		Reduce input groups=8
		Reduce shuffle bytes=1372
		Reduce input records=8
		Reduce output records=8
		Spilled Records=16
		Shuffled Maps =82
		Failed Shuffles=0
		Merged Map outputs=82
		GC time elapsed (ms)=19278
		CPU time spent (ms)=426740
		Physical memory (bytes) snapshot=22411476992
		Virtual memory (bytes) snapshot=76006252544
		Total committed heap usage (bytes)=16409690112
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	org.apache.hadoop.examples.WorkGen$Counters
		MAP_BYTES_WRITTEN=800
		MAP_RECORDS_WRITTEN=8
		RED_BYTES_WRITTEN=800
		RED_RECORDS_WRITTEN=8
	File Input Format Counters 
		Bytes Read=3222341372
	File Output Format Counters 
		Bytes Written=1024
Job ended: Thu Aug 17 22:10:00 MDT 2017
The job took 426 seconds.
