17/08/17 04:58:35 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/08/17 04:58:35 INFO client.RMProxy: Connecting to ResourceManager at node-1/10.1.1.3:8032
Max number of map tasks 150
Max number of red tasks 30
shuffleInputRatio  = 1.6408635
outputShuffleRatio = 0.60875624
Running on 15 nodes with 135 maps and 3 reduces.
0.3742282147460704
0.3305827870056811
Job started: Thu Aug 17 04:58:36 MDT 2017
17/08/17 04:58:36 INFO client.RMProxy: Connecting to ResourceManager at node-1/10.1.1.3:8032
17/08/17 04:58:36 INFO client.RMProxy: Connecting to ResourceManager at node-1/10.1.1.3:8032
17/08/17 04:58:37 INFO hdfs.DFSClient: @huanke messageHK: TimeHK: 59 MessageHK 
17/08/17 04:58:37 INFO hdfs.DFSClient: @Cesar: Pipe nodes are {0=pc747.emulab.net, 1=pc824.emulab.net}
17/08/17 04:58:37 INFO hdfs.DFSClient: @Cesar: Processing ack seqno: 0 reply: SUCCESS reply: SUCCESS downstreamAckTimeNanos: 643046 flag: 0 flag: 0 ackTime: 1502967517357 ackTime: 1502967517357 transferTimeToNext: "0$$pc747.emulab.net$0" transferTimeToNext: "-" acumulatedTime: 39 acumulatedTime: 39 containing 2 timestamps
17/08/17 04:58:37 INFO hdfs.DFSClient: @Cesar: Processing packet 0 on DatanodeInfoWithStorage[10.1.1.18:50010,DS-15b6d8de-5f30-4249-b19a-96e847aa299f,DISK] with status: SUCCESS and timestamp 1502967517357
17/08/17 04:58:37 INFO mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
17/08/17 04:58:37 INFO mortbay.log: @Cesar: Streamer is Thread[DataStreamer for file /tmp/hadoop-yarn/staging/hsgucare/.staging/job_1502932926037_0121/job.jar block BP-2096791522-155.98.36.121-1502932830505:blk_1073747113_6289,5,main]
17/08/17 04:58:37 INFO hdfs.DFSClient: @Cesar: Processing packet 0 on DatanodeInfoWithStorage[10.1.1.11:50010,DS-6920534a-fa96-4c4a-90f9-9c0a9faea38b,DISK] with status: SUCCESS and timestamp 1502967517357
17/08/17 04:58:37 INFO mortbay.log: @Cesar: Streamer is Thread[DataStreamer for file /tmp/hadoop-yarn/staging/hsgucare/.staging/job_1502932926037_0121/job.jar block BP-2096791522-155.98.36.121-1502932830505:blk_1073747113_6289,5,main]
17/08/17 04:58:37 INFO hdfs.DFSClient: @Cesar: Acum time: {pc824.emulab.net=HdfsWriteData [bytesWritten=8255, elapsedNanos=39], pc747.emulab.net=HdfsWriteData [bytesWritten=8255, elapsedNanos=39]}
17/08/17 04:58:37 INFO hdfs.DFSClient: @Cesar: Processing ack seqno: 1 reply: SUCCESS reply: SUCCESS downstreamAckTimeNanos: 1043303 flag: 0 flag: 0 ackTime: 1502967517494 ackTime: 1502967517495 transferTimeToNext: "1$$pc747.emulab.net$0" transferTimeToNext: "-" acumulatedTime: 137 acumulatedTime: 138 containing 2 timestamps
17/08/17 04:58:37 INFO hdfs.DFSClient: @Cesar: Processing packet 1 on DatanodeInfoWithStorage[10.1.1.18:50010,DS-15b6d8de-5f30-4249-b19a-96e847aa299f,DISK] with status: SUCCESS and timestamp 1502967517494
17/08/17 04:58:37 INFO mortbay.log: @Cesar: Streamer is Thread[DataStreamer for file /tmp/hadoop-yarn/staging/hsgucare/.staging/job_1502932926037_0121/job.jar block BP-2096791522-155.98.36.121-1502932830505:blk_1073747113_6289,5,main]
17/08/17 04:58:37 INFO hdfs.DFSClient: @Cesar: Processing packet 1 on DatanodeInfoWithStorage[10.1.1.11:50010,DS-6920534a-fa96-4c4a-90f9-9c0a9faea38b,DISK] with status: SUCCESS and timestamp 1502967517495
17/08/17 04:58:37 INFO mortbay.log: @Cesar: Streamer is Thread[DataStreamer for file /tmp/hadoop-yarn/staging/hsgucare/.staging/job_1502932926037_0121/job.jar block BP-2096791522-155.98.36.121-1502932830505:blk_1073747113_6289,5,main]
17/08/17 04:58:37 INFO hdfs.DFSClient: @Cesar: Acum time: {pc824.emulab.net=HdfsWriteData [bytesWritten=8259, elapsedNanos=177], pc747.emulab.net=HdfsWriteData [bytesWritten=8259, elapsedNanos=176]}
17/08/17 04:58:37 INFO mapred.FileInputFormat: Total input paths to process : 69
17/08/17 04:58:37 INFO hdfs.DFSClient: @huanke messageHK: TimeHK: 24 MessageHK 
17/08/17 04:58:37 INFO hdfs.DFSClient: @Cesar: Pipe nodes are {6=pc846.emulab.net, 5=pc829.emulab.net, 0=pc845.emulab.net, 7=pc760.emulab.net, 8=pc824.emulab.net, 2=pc717.emulab.net, 9=pc747.emulab.net, 1=pc843.emulab.net, 3=pc718.emulab.net, 4=pc825.emulab.net}
17/08/17 04:58:37 INFO hdfs.DFSClient: @Cesar: Processing ack seqno: 0 reply: SUCCESS reply: SUCCESS reply: SUCCESS reply: SUCCESS reply: SUCCESS reply: SUCCESS reply: SUCCESS reply: SUCCESS reply: SUCCESS reply: SUCCESS downstreamAckTimeNanos: 6132598 flag: 0 flag: 0 flag: 0 flag: 0 flag: 0 flag: 0 flag: 0 flag: 0 flag: 0 flag: 0 ackTime: 1502967517983 ackTime: 1502967517977 ackTime: 1502967517976 ackTime: 1502967517978 ackTime: 1502967517979 ackTime: 1502967517979 ackTime: 1502967517979 ackTime: 1502967517980 ackTime: 1502967517980 ackTime: 1502967517981 transferTimeToNext: "0$$pc845.emulab.net$0" transferTimeToNext: "0$$pc843.emulab.net$0" transferTimeToNext: "0$$pc717.emulab.net$0" transferTimeToNext: "0$$pc718.emulab.net$0" transferTimeToNext: "0$$pc825.emulab.net$0" transferTimeToNext: "0$$pc829.emulab.net$0" transferTimeToNext: "0$$pc846.emulab.net$0" transferTimeToNext: "0$$pc760.emulab.net$0" transferTimeToNext: "0$$pc824.emulab.net$0" transferTimeToNext: "-" acumulatedTime: 1 acumulatedTime: 2 acumulatedTime: 2 acumulatedTime: 3 acumulatedTime: 4 acumulatedTime: 4 acumulatedTime: 4 acumulatedTime: 5 acumulatedTime: 6 acumulatedTime: 7 containing 10 timestamps
17/08/17 04:58:37 INFO hdfs.DFSClient: @Cesar: Processing packet 0 on DatanodeInfoWithStorage[10.1.1.16:50010,DS-88996a2d-ca56-4720-a6a5-27b3ed5faec6,DISK] with status: SUCCESS and timestamp 1502967517983
17/08/17 04:58:37 INFO mortbay.log: @Cesar: Streamer is Thread[DataStreamer for file /tmp/hadoop-yarn/staging/hsgucare/.staging/job_1502932926037_0121/job.split block BP-2096791522-155.98.36.121-1502932830505:blk_1073747114_6290,5,main]
17/08/17 04:58:37 INFO hdfs.DFSClient: @Cesar: Processing packet 0 on DatanodeInfoWithStorage[10.1.1.14:50010,DS-7219591e-d26d-4053-8b84-344273d88eca,DISK] with status: SUCCESS and timestamp 1502967517977
17/08/17 04:58:37 INFO mortbay.log: @Cesar: Streamer is Thread[DataStreamer for file /tmp/hadoop-yarn/staging/hsgucare/.staging/job_1502932926037_0121/job.split block BP-2096791522-155.98.36.121-1502932830505:blk_1073747114_6290,5,main]
17/08/17 04:58:37 INFO hdfs.DFSClient: @Cesar: Processing packet 0 on DatanodeInfoWithStorage[10.1.1.8:50010,DS-656c8af1-02ab-45ef-92c6-1b776f161d87,DISK] with status: SUCCESS and timestamp 1502967517976
17/08/17 04:58:37 INFO mortbay.log: @Cesar: Streamer is Thread[DataStreamer for file /tmp/hadoop-yarn/staging/hsgucare/.staging/job_1502932926037_0121/job.split block BP-2096791522-155.98.36.121-1502932830505:blk_1073747114_6290,5,main]
17/08/17 04:58:37 INFO hdfs.DFSClient: @Cesar: Processing packet 0 on DatanodeInfoWithStorage[10.1.1.7:50010,DS-cd7343b9-9cfd-425a-b691-8e318b92dfad,DISK] with status: SUCCESS and timestamp 1502967517978
17/08/17 04:58:37 INFO mortbay.log: @Cesar: Streamer is Thread[DataStreamer for file /tmp/hadoop-yarn/staging/hsgucare/.staging/job_1502932926037_0121/job.split block BP-2096791522-155.98.36.121-1502932830505:blk_1073747114_6290,5,main]
17/08/17 04:58:37 INFO hdfs.DFSClient: @Cesar: Processing packet 0 on DatanodeInfoWithStorage[10.1.1.17:50010,DS-0d9c0cb8-8c4e-4152-9850-abc93c744b7a,DISK] with status: SUCCESS and timestamp 1502967517979
17/08/17 04:58:37 INFO mortbay.log: @Cesar: Streamer is Thread[DataStreamer for file /tmp/hadoop-yarn/staging/hsgucare/.staging/job_1502932926037_0121/job.split block BP-2096791522-155.98.36.121-1502932830505:blk_1073747114_6290,5,main]
17/08/17 04:58:37 INFO hdfs.DFSClient: @Cesar: Processing packet 0 on DatanodeInfoWithStorage[10.1.1.9:50010,DS-9a2de4b9-9196-4f0b-a1cb-3ecf63da8a40,DISK] with status: SUCCESS and timestamp 1502967517979
17/08/17 04:58:37 INFO mortbay.log: @Cesar: Streamer is Thread[DataStreamer for file /tmp/hadoop-yarn/staging/hsgucare/.staging/job_1502932926037_0121/job.split block BP-2096791522-155.98.36.121-1502932830505:blk_1073747114_6290,5,main]
17/08/17 04:58:37 INFO hdfs.DFSClient: @Cesar: Processing packet 0 on DatanodeInfoWithStorage[10.1.1.10:50010,DS-f2e332dc-695f-4d0b-a679-62802b8f8469,DISK] with status: SUCCESS and timestamp 1502967517979
17/08/17 04:58:37 INFO mortbay.log: @Cesar: Streamer is Thread[DataStreamer for file /tmp/hadoop-yarn/staging/hsgucare/.staging/job_1502932926037_0121/job.split block BP-2096791522-155.98.36.121-1502932830505:blk_1073747114_6290,5,main]
17/08/17 04:58:37 INFO hdfs.DFSClient: @Cesar: Processing packet 0 on DatanodeInfoWithStorage[10.1.1.12:50010,DS-ab640d6c-43cf-49c9-ac6e-96eadb4cc9bd,DISK] with status: SUCCESS and timestamp 1502967517980
17/08/17 04:58:37 INFO mortbay.log: @Cesar: Streamer is Thread[DataStreamer for file /tmp/hadoop-yarn/staging/hsgucare/.staging/job_1502932926037_0121/job.split block BP-2096791522-155.98.36.121-1502932830505:blk_1073747114_6290,5,main]
17/08/17 04:58:37 INFO hdfs.DFSClient: @Cesar: Processing packet 0 on DatanodeInfoWithStorage[10.1.1.11:50010,DS-6920534a-fa96-4c4a-90f9-9c0a9faea38b,DISK] with status: SUCCESS and timestamp 1502967517980
17/08/17 04:58:37 INFO mortbay.log: @Cesar: Streamer is Thread[DataStreamer for file /tmp/hadoop-yarn/staging/hsgucare/.staging/job_1502932926037_0121/job.split block BP-2096791522-155.98.36.121-1502932830505:blk_1073747114_6290,5,main]
17/08/17 04:58:37 INFO hdfs.DFSClient: @Cesar: Processing packet 0 on DatanodeInfoWithStorage[10.1.1.18:50010,DS-15b6d8de-5f30-4249-b19a-96e847aa299f,DISK] with status: SUCCESS and timestamp 1502967517981
17/08/17 04:58:37 INFO mortbay.log: @Cesar: Streamer is Thread[DataStreamer for file /tmp/hadoop-yarn/staging/hsgucare/.staging/job_1502932926037_0121/job.split block BP-2096791522-155.98.36.121-1502932830505:blk_1073747114_6290,5,main]
17/08/17 04:58:37 INFO hdfs.DFSClient: @Cesar: Acum time: {pc824.emulab.net=HdfsWriteData [bytesWritten=15035, elapsedNanos=6], pc829.emulab.net=HdfsWriteData [bytesWritten=15035, elapsedNanos=4], pc718.emulab.net=HdfsWriteData [bytesWritten=15035, elapsedNanos=3], pc760.emulab.net=HdfsWriteData [bytesWritten=15035, elapsedNanos=5], pc846.emulab.net=HdfsWriteData [bytesWritten=15035, elapsedNanos=4], pc717.emulab.net=HdfsWriteData [bytesWritten=15035, elapsedNanos=2], pc843.emulab.net=HdfsWriteData [bytesWritten=15035, elapsedNanos=2], pc845.emulab.net=HdfsWriteData [bytesWritten=15035, elapsedNanos=1], pc825.emulab.net=HdfsWriteData [bytesWritten=15035, elapsedNanos=4], pc747.emulab.net=HdfsWriteData [bytesWritten=15035, elapsedNanos=7]}
17/08/17 04:58:37 INFO hdfs.DFSClient: @Cesar: Processing ack seqno: 1 reply: SUCCESS reply: SUCCESS reply: SUCCESS reply: SUCCESS reply: SUCCESS reply: SUCCESS reply: SUCCESS reply: SUCCESS reply: SUCCESS reply: SUCCESS downstreamAckTimeNanos: 7404260 flag: 0 flag: 0 flag: 0 flag: 0 flag: 0 flag: 0 flag: 0 flag: 0 flag: 0 flag: 0 ackTime: 1502967517995 ackTime: 1502967517988 ackTime: 1502967517987 ackTime: 1502967517989 ackTime: 1502967517989 ackTime: 1502967517989 ackTime: 1502967517989 ackTime: 1502967517989 ackTime: 1502967517989 ackTime: 1502967517989 transferTimeToNext: "1$$pc845.emulab.net$0" transferTimeToNext: "1$$pc843.emulab.net$0" transferTimeToNext: "1$$pc717.emulab.net$0" transferTimeToNext: "1$$pc718.emulab.net$0" transferTimeToNext: "1$$pc825.emulab.net$0" transferTimeToNext: "1$$pc829.emulab.net$0" transferTimeToNext: "1$$pc846.emulab.net$0" transferTimeToNext: "1$$pc760.emulab.net$0" transferTimeToNext: "1$$pc824.emulab.net$0" transferTimeToNext: "-" acumulatedTime: 11 acumulatedTime: 11 acumulatedTime: 10 acumulatedTime: 11 acumulatedTime: 10 acumulatedTime: 10 acumulatedTime: 10 acumulatedTime: 9 acumulatedTime: 9 acumulatedTime: 8 containing 10 timestamps
17/08/17 04:58:37 INFO hdfs.DFSClient: @Cesar: Processing packet 1 on DatanodeInfoWithStorage[10.1.1.16:50010,DS-88996a2d-ca56-4720-a6a5-27b3ed5faec6,DISK] with status: SUCCESS and timestamp 1502967517995
17/08/17 04:58:37 INFO mortbay.log: @Cesar: Streamer is Thread[DataStreamer for file /tmp/hadoop-yarn/staging/hsgucare/.staging/job_1502932926037_0121/job.split block BP-2096791522-155.98.36.121-1502932830505:blk_1073747114_6290,5,main]
17/08/17 04:58:37 INFO hdfs.DFSClient: @Cesar: Processing packet 1 on DatanodeInfoWithStorage[10.1.1.14:50010,DS-7219591e-d26d-4053-8b84-344273d88eca,DISK] with status: SUCCESS and timestamp 1502967517988
17/08/17 04:58:37 INFO mortbay.log: @Cesar: Streamer is Thread[DataStreamer for file /tmp/hadoop-yarn/staging/hsgucare/.staging/job_1502932926037_0121/job.split block BP-2096791522-155.98.36.121-1502932830505:blk_1073747114_6290,5,main]
17/08/17 04:58:37 INFO hdfs.DFSClient: @Cesar: Processing packet 1 on DatanodeInfoWithStorage[10.1.1.8:50010,DS-656c8af1-02ab-45ef-92c6-1b776f161d87,DISK] with status: SUCCESS and timestamp 1502967517987
17/08/17 04:58:37 INFO mortbay.log: @Cesar: Streamer is Thread[DataStreamer for file /tmp/hadoop-yarn/staging/hsgucare/.staging/job_1502932926037_0121/job.split block BP-2096791522-155.98.36.121-1502932830505:blk_1073747114_6290,5,main]
17/08/17 04:58:37 INFO hdfs.DFSClient: @Cesar: Processing packet 1 on DatanodeInfoWithStorage[10.1.1.7:50010,DS-cd7343b9-9cfd-425a-b691-8e318b92dfad,DISK] with status: SUCCESS and timestamp 1502967517989
17/08/17 04:58:37 INFO mortbay.log: @Cesar: Streamer is Thread[DataStreamer for file /tmp/hadoop-yarn/staging/hsgucare/.staging/job_1502932926037_0121/job.split block BP-2096791522-155.98.36.121-1502932830505:blk_1073747114_6290,5,main]
17/08/17 04:58:38 INFO hdfs.DFSClient: @Cesar: Processing packet 1 on DatanodeInfoWithStorage[10.1.1.17:50010,DS-0d9c0cb8-8c4e-4152-9850-abc93c744b7a,DISK] with status: SUCCESS and timestamp 1502967517989
17/08/17 04:58:38 INFO mortbay.log: @Cesar: Streamer is Thread[DataStreamer for file /tmp/hadoop-yarn/staging/hsgucare/.staging/job_1502932926037_0121/job.split block BP-2096791522-155.98.36.121-1502932830505:blk_1073747114_6290,5,main]
17/08/17 04:58:38 INFO hdfs.DFSClient: @Cesar: Processing packet 1 on DatanodeInfoWithStorage[10.1.1.9:50010,DS-9a2de4b9-9196-4f0b-a1cb-3ecf63da8a40,DISK] with status: SUCCESS and timestamp 1502967517989
17/08/17 04:58:38 INFO mortbay.log: @Cesar: Streamer is Thread[DataStreamer for file /tmp/hadoop-yarn/staging/hsgucare/.staging/job_1502932926037_0121/job.split block BP-2096791522-155.98.36.121-1502932830505:blk_1073747114_6290,5,main]
17/08/17 04:58:38 INFO hdfs.DFSClient: @Cesar: Processing packet 1 on DatanodeInfoWithStorage[10.1.1.10:50010,DS-f2e332dc-695f-4d0b-a679-62802b8f8469,DISK] with status: SUCCESS and timestamp 1502967517989
17/08/17 04:58:38 INFO mortbay.log: @Cesar: Streamer is Thread[DataStreamer for file /tmp/hadoop-yarn/staging/hsgucare/.staging/job_1502932926037_0121/job.split block BP-2096791522-155.98.36.121-1502932830505:blk_1073747114_6290,5,main]
17/08/17 04:58:38 INFO hdfs.DFSClient: @Cesar: Processing packet 1 on DatanodeInfoWithStorage[10.1.1.12:50010,DS-ab640d6c-43cf-49c9-ac6e-96eadb4cc9bd,DISK] with status: SUCCESS and timestamp 1502967517989
17/08/17 04:58:38 INFO mortbay.log: @Cesar: Streamer is Thread[DataStreamer for file /tmp/hadoop-yarn/staging/hsgucare/.staging/job_1502932926037_0121/job.split block BP-2096791522-155.98.36.121-1502932830505:blk_1073747114_6290,5,main]
17/08/17 04:58:38 INFO hdfs.DFSClient: @Cesar: Processing packet 1 on DatanodeInfoWithStorage[10.1.1.11:50010,DS-6920534a-fa96-4c4a-90f9-9c0a9faea38b,DISK] with status: SUCCESS and timestamp 1502967517989
17/08/17 04:58:38 INFO mortbay.log: @Cesar: Streamer is Thread[DataStreamer for file /tmp/hadoop-yarn/staging/hsgucare/.staging/job_1502932926037_0121/job.split block BP-2096791522-155.98.36.121-1502932830505:blk_1073747114_6290,5,main]
17/08/17 04:58:38 INFO hdfs.DFSClient: @Cesar: Processing packet 1 on DatanodeInfoWithStorage[10.1.1.18:50010,DS-15b6d8de-5f30-4249-b19a-96e847aa299f,DISK] with status: SUCCESS and timestamp 1502967517989
17/08/17 04:58:38 INFO mortbay.log: @Cesar: Streamer is Thread[DataStreamer for file /tmp/hadoop-yarn/staging/hsgucare/.staging/job_1502932926037_0121/job.split block BP-2096791522-155.98.36.121-1502932830505:blk_1073747114_6290,5,main]
17/08/17 04:58:38 INFO hdfs.DFSClient: @Cesar: Acum time: {pc824.emulab.net=HdfsWriteData [bytesWritten=15039, elapsedNanos=15], pc829.emulab.net=HdfsWriteData [bytesWritten=15039, elapsedNanos=14], pc718.emulab.net=HdfsWriteData [bytesWritten=15039, elapsedNanos=14], pc760.emulab.net=HdfsWriteData [bytesWritten=15039, elapsedNanos=14], pc846.emulab.net=HdfsWriteData [bytesWritten=15039, elapsedNanos=14], pc717.emulab.net=HdfsWriteData [bytesWritten=15039, elapsedNanos=12], pc843.emulab.net=HdfsWriteData [bytesWritten=15039, elapsedNanos=13], pc845.emulab.net=HdfsWriteData [bytesWritten=15039, elapsedNanos=12], pc825.emulab.net=HdfsWriteData [bytesWritten=15039, elapsedNanos=14], pc747.emulab.net=HdfsWriteData [bytesWritten=15039, elapsedNanos=15]}
17/08/17 04:58:38 INFO hdfs.DFSClient: @huanke messageHK: TimeHK: 4 MessageHK 
17/08/17 04:58:38 INFO hdfs.DFSClient: @Cesar: Pipe nodes are {0=pc745.emulab.net, 1=pc760.emulab.net}
17/08/17 04:58:38 INFO hdfs.DFSClient: @Cesar: Processing ack seqno: 0 reply: SUCCESS reply: SUCCESS downstreamAckTimeNanos: 606262 flag: 0 flag: 0 ackTime: 1502967518029 ackTime: 1502967518030 transferTimeToNext: "0$$pc745.emulab.net$0" transferTimeToNext: "-" acumulatedTime: 1 acumulatedTime: 1 containing 2 timestamps
17/08/17 04:58:38 INFO hdfs.DFSClient: @Cesar: Processing packet 0 on DatanodeInfoWithStorage[10.1.1.6:50010,DS-b25433fd-e632-4fe0-819d-88b5346f95f3,DISK] with status: SUCCESS and timestamp 1502967518029
17/08/17 04:58:38 INFO mortbay.log: @Cesar: Streamer is Thread[DataStreamer for file /tmp/hadoop-yarn/staging/hsgucare/.staging/job_1502932926037_0121/job.splitmetainfo block BP-2096791522-155.98.36.121-1502932830505:blk_1073747115_6291,5,main]
17/08/17 04:58:38 INFO hdfs.DFSClient: @Cesar: Processing packet 0 on DatanodeInfoWithStorage[10.1.1.12:50010,DS-ab640d6c-43cf-49c9-ac6e-96eadb4cc9bd,DISK] with status: SUCCESS and timestamp 1502967518030
17/08/17 04:58:38 INFO mortbay.log: @Cesar: Streamer is Thread[DataStreamer for file /tmp/hadoop-yarn/staging/hsgucare/.staging/job_1502932926037_0121/job.splitmetainfo block BP-2096791522-155.98.36.121-1502932830505:blk_1073747115_6291,5,main]
17/08/17 04:58:38 INFO hdfs.DFSClient: @Cesar: Acum time: {pc760.emulab.net=HdfsWriteData [bytesWritten=5923, elapsedNanos=1], pc745.emulab.net=HdfsWriteData [bytesWritten=5923, elapsedNanos=1]}
17/08/17 04:58:38 INFO hdfs.DFSClient: @Cesar: Processing ack seqno: 1 reply: SUCCESS reply: SUCCESS downstreamAckTimeNanos: 820739 flag: 0 flag: 0 ackTime: 1502967518032 ackTime: 1502967518033 transferTimeToNext: "1$$pc745.emulab.net$0" transferTimeToNext: "-" acumulatedTime: 3 acumulatedTime: 3 containing 2 timestamps
17/08/17 04:58:38 INFO hdfs.DFSClient: @Cesar: Processing packet 1 on DatanodeInfoWithStorage[10.1.1.6:50010,DS-b25433fd-e632-4fe0-819d-88b5346f95f3,DISK] with status: SUCCESS and timestamp 1502967518032
17/08/17 04:58:38 INFO mortbay.log: @Cesar: Streamer is Thread[DataStreamer for file /tmp/hadoop-yarn/staging/hsgucare/.staging/job_1502932926037_0121/job.splitmetainfo block BP-2096791522-155.98.36.121-1502932830505:blk_1073747115_6291,5,main]
17/08/17 04:58:38 INFO hdfs.DFSClient: @Cesar: Processing packet 1 on DatanodeInfoWithStorage[10.1.1.12:50010,DS-ab640d6c-43cf-49c9-ac6e-96eadb4cc9bd,DISK] with status: SUCCESS and timestamp 1502967518033
17/08/17 04:58:38 INFO mortbay.log: @Cesar: Streamer is Thread[DataStreamer for file /tmp/hadoop-yarn/staging/hsgucare/.staging/job_1502932926037_0121/job.splitmetainfo block BP-2096791522-155.98.36.121-1502932830505:blk_1073747115_6291,5,main]
17/08/17 04:58:38 INFO hdfs.DFSClient: @Cesar: Acum time: {pc760.emulab.net=HdfsWriteData [bytesWritten=5927, elapsedNanos=4], pc745.emulab.net=HdfsWriteData [bytesWritten=5927, elapsedNanos=4]}
17/08/17 04:58:38 INFO mapreduce.JobSubmitter: number of splits:138
17/08/17 04:58:38 INFO hdfs.DFSClient: @huanke messageHK: TimeHK: 4 MessageHK 
17/08/17 04:58:38 INFO hdfs.DFSClient: @Cesar: Pipe nodes are {0=pc843.emulab.net, 1=pc857.emulab.net}
17/08/17 04:58:38 INFO hdfs.DFSClient: @Cesar: Processing ack seqno: 0 reply: SUCCESS reply: SUCCESS downstreamAckTimeNanos: 1403255 flag: 0 flag: 0 ackTime: 1502967518372 ackTime: 1502967518373 transferTimeToNext: "0$$pc843.emulab.net$0" transferTimeToNext: "-" acumulatedTime: 2 acumulatedTime: 3 containing 2 timestamps
17/08/17 04:58:38 INFO hdfs.DFSClient: @Cesar: Processing packet 0 on DatanodeInfoWithStorage[10.1.1.14:50010,DS-7219591e-d26d-4053-8b84-344273d88eca,DISK] with status: SUCCESS and timestamp 1502967518372
17/08/17 04:58:38 INFO mortbay.log: @Cesar: Streamer is Thread[DataStreamer for file /tmp/hadoop-yarn/staging/hsgucare/.staging/job_1502932926037_0121/job.xml block BP-2096791522-155.98.36.121-1502932830505:blk_1073747117_6293,5,main]
17/08/17 04:58:38 INFO hdfs.DFSClient: @Cesar: Processing packet 0 on DatanodeInfoWithStorage[10.1.1.15:50010,DS-2eab3ca3-fe3b-47dd-b8e9-a4212b02f1e3,DISK] with status: SUCCESS and timestamp 1502967518373
17/08/17 04:58:38 INFO mortbay.log: @Cesar: Streamer is Thread[DataStreamer for file /tmp/hadoop-yarn/staging/hsgucare/.staging/job_1502932926037_0121/job.xml block BP-2096791522-155.98.36.121-1502932830505:blk_1073747117_6293,5,main]
17/08/17 04:58:38 INFO hdfs.DFSClient: @Cesar: Acum time: {pc857.emulab.net=HdfsWriteData [bytesWritten=65020, elapsedNanos=3], pc843.emulab.net=HdfsWriteData [bytesWritten=65020, elapsedNanos=2]}
17/08/17 04:58:38 INFO hdfs.DFSClient: @Cesar: Processing ack seqno: 1 reply: SUCCESS reply: SUCCESS downstreamAckTimeNanos: 823840 flag: 0 flag: 0 ackTime: 1502967518410 ackTime: 1502967518411 transferTimeToNext: "1$$pc843.emulab.net$0" transferTimeToNext: "-" acumulatedTime: 38 acumulatedTime: 37 containing 2 timestamps
17/08/17 04:58:38 INFO hdfs.DFSClient: @Cesar: Processing packet 1 on DatanodeInfoWithStorage[10.1.1.14:50010,DS-7219591e-d26d-4053-8b84-344273d88eca,DISK] with status: SUCCESS and timestamp 1502967518410
17/08/17 04:58:38 INFO mortbay.log: @Cesar: Streamer is Thread[DataStreamer for file /tmp/hadoop-yarn/staging/hsgucare/.staging/job_1502932926037_0121/job.xml block BP-2096791522-155.98.36.121-1502932830505:blk_1073747117_6293,5,main]
17/08/17 04:58:38 INFO hdfs.DFSClient: @Cesar: Processing packet 1 on DatanodeInfoWithStorage[10.1.1.15:50010,DS-2eab3ca3-fe3b-47dd-b8e9-a4212b02f1e3,DISK] with status: SUCCESS and timestamp 1502967518411
17/08/17 04:58:38 INFO mortbay.log: @Cesar: Streamer is Thread[DataStreamer for file /tmp/hadoop-yarn/staging/hsgucare/.staging/job_1502932926037_0121/job.xml block BP-2096791522-155.98.36.121-1502932830505:blk_1073747117_6293,5,main]
17/08/17 04:58:38 INFO hdfs.DFSClient: @Cesar: Acum time: {pc857.emulab.net=HdfsWriteData [bytesWritten=102251, elapsedNanos=40], pc843.emulab.net=HdfsWriteData [bytesWritten=102251, elapsedNanos=40]}
17/08/17 04:58:38 INFO hdfs.DFSClient: @Cesar: Processing ack seqno: 2 reply: SUCCESS reply: SUCCESS downstreamAckTimeNanos: 702813 flag: 0 flag: 0 ackTime: 1502967518413 ackTime: 1502967518413 transferTimeToNext: "2$$pc843.emulab.net$0" transferTimeToNext: "-" acumulatedTime: 3 acumulatedTime: 2 containing 2 timestamps
17/08/17 04:58:38 INFO hdfs.DFSClient: @Cesar: Processing packet 2 on DatanodeInfoWithStorage[10.1.1.14:50010,DS-7219591e-d26d-4053-8b84-344273d88eca,DISK] with status: SUCCESS and timestamp 1502967518413
17/08/17 04:58:38 INFO mortbay.log: @Cesar: Streamer is Thread[DataStreamer for file /tmp/hadoop-yarn/staging/hsgucare/.staging/job_1502932926037_0121/job.xml block BP-2096791522-155.98.36.121-1502932830505:blk_1073747117_6293,5,main]
17/08/17 04:58:38 INFO hdfs.DFSClient: @Cesar: Processing packet 2 on DatanodeInfoWithStorage[10.1.1.15:50010,DS-2eab3ca3-fe3b-47dd-b8e9-a4212b02f1e3,DISK] with status: SUCCESS and timestamp 1502967518413
17/08/17 04:58:38 INFO mortbay.log: @Cesar: Streamer is Thread[DataStreamer for file /tmp/hadoop-yarn/staging/hsgucare/.staging/job_1502932926037_0121/job.xml block BP-2096791522-155.98.36.121-1502932830505:blk_1073747117_6293,5,main]
17/08/17 04:58:38 INFO hdfs.DFSClient: @Cesar: Acum time: {pc857.emulab.net=HdfsWriteData [bytesWritten=102255, elapsedNanos=42], pc843.emulab.net=HdfsWriteData [bytesWritten=102255, elapsedNanos=43]}
17/08/17 04:58:38 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1502932926037_0121
17/08/17 04:58:38 INFO impl.YarnClientImpl: Submitted application application_1502932926037_0121
17/08/17 04:58:38 INFO mapreduce.Job: The url to track the job: http://node-1:8088/proxy/application_1502932926037_0121/
17/08/17 04:58:38 INFO mapreduce.Job: Running job: job_1502932926037_0121
17/08/17 05:00:26 INFO mapreduce.Job: Job job_1502932926037_0121 running in uber mode : false
17/08/17 05:00:26 INFO mapreduce.Job:  map 0% reduce 0%
17/08/17 05:00:35 INFO mapreduce.Job:  map 38% reduce 0%
17/08/17 05:00:36 INFO mapreduce.Job:  map 48% reduce 0%
17/08/17 05:00:38 INFO mapreduce.Job:  map 51% reduce 0%
17/08/17 05:00:39 INFO mapreduce.Job:  map 79% reduce 0%
17/08/17 05:00:42 INFO mapreduce.Job:  map 88% reduce 0%
17/08/17 05:00:43 INFO mapreduce.Job:  map 94% reduce 0%
17/08/17 05:00:44 INFO mapreduce.Job:  map 99% reduce 0%
17/08/17 05:00:45 INFO mapreduce.Job:  map 100% reduce 0%
17/08/17 05:00:47 INFO mapreduce.Job:  map 100% reduce 3%
17/08/17 05:00:48 INFO mapreduce.Job: Task Id : attempt_1502932926037_0121_r_000001_0, Status : FAILED
Error: org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#1
	at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:153)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:431)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
Caused by: java.lang.OutOfMemoryError: Java heap space
	at org.apache.hadoop.io.BoundedByteArrayOutputStream.<init>(BoundedByteArrayOutputStream.java:56)
	at org.apache.hadoop.io.BoundedByteArrayOutputStream.<init>(BoundedByteArrayOutputStream.java:46)
	at org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput.<init>(InMemoryMapOutput.java:64)
	at org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.unconditionalReserve(MergeManagerImpl.java:304)
	at org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.reserve(MergeManagerImpl.java:294)
	at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyMapOutput(Fetcher.java:547)
	at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:369)
	at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:217)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

17/08/17 05:00:48 INFO mapreduce.Job: Task Id : attempt_1502932926037_0121_r_000000_0, Status : FAILED
Error: org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#2
	at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:153)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:431)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
Caused by: java.lang.OutOfMemoryError: Java heap space
	at org.apache.hadoop.io.BoundedByteArrayOutputStream.<init>(BoundedByteArrayOutputStream.java:56)
	at org.apache.hadoop.io.BoundedByteArrayOutputStream.<init>(BoundedByteArrayOutputStream.java:46)
	at org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput.<init>(InMemoryMapOutput.java:64)
	at org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.unconditionalReserve(MergeManagerImpl.java:304)
	at org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.reserve(MergeManagerImpl.java:294)
	at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyMapOutput(Fetcher.java:547)
	at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:369)
	at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:217)

Container killed by the ApplicationMaster.
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143

17/08/17 05:00:51 INFO mapreduce.Job:  map 100% reduce 4%
17/08/17 05:00:54 INFO mapreduce.Job:  map 100% reduce 5%
17/08/17 05:00:58 INFO mapreduce.Job:  map 100% reduce 9%
17/08/17 05:01:00 INFO mapreduce.Job:  map 100% reduce 10%
17/08/17 05:01:01 INFO mapreduce.Job:  map 100% reduce 11%
17/08/17 05:01:03 INFO mapreduce.Job:  map 100% reduce 12%
17/08/17 05:01:05 INFO mapreduce.Job:  map 100% reduce 13%
17/08/17 05:01:06 INFO mapreduce.Job:  map 100% reduce 14%
17/08/17 05:01:09 INFO mapreduce.Job:  map 100% reduce 15%
17/08/17 05:01:11 INFO mapreduce.Job:  map 100% reduce 16%
17/08/17 05:01:14 INFO mapreduce.Job:  map 100% reduce 18%
17/08/17 05:01:17 INFO mapreduce.Job:  map 100% reduce 19%
17/08/17 05:01:19 INFO mapreduce.Job:  map 100% reduce 20%
17/08/17 05:01:20 INFO mapreduce.Job:  map 100% reduce 21%
17/08/17 05:01:24 INFO mapreduce.Job:  map 100% reduce 22%
17/08/17 05:01:25 INFO mapreduce.Job:  map 100% reduce 23%
17/08/17 05:01:27 INFO mapreduce.Job:  map 100% reduce 24%
17/08/17 05:01:30 INFO mapreduce.Job:  map 100% reduce 25%
17/08/17 05:01:33 INFO mapreduce.Job:  map 100% reduce 26%
17/08/17 05:01:35 INFO mapreduce.Job:  map 100% reduce 27%
17/08/17 05:01:36 INFO mapreduce.Job:  map 100% reduce 28%
17/08/17 05:01:38 INFO mapreduce.Job: Task Id : attempt_1502932926037_0121_r_000002_0, Status : FAILED
Error: org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#2
	at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:153)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:431)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
Caused by: java.lang.OutOfMemoryError: Java heap space
	at org.apache.hadoop.io.BoundedByteArrayOutputStream.<init>(BoundedByteArrayOutputStream.java:56)
	at org.apache.hadoop.io.BoundedByteArrayOutputStream.<init>(BoundedByteArrayOutputStream.java:46)
	at org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput.<init>(InMemoryMapOutput.java:64)
	at org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.unconditionalReserve(MergeManagerImpl.java:304)
	at org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.reserve(MergeManagerImpl.java:294)
	at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyMapOutput(Fetcher.java:547)
	at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:369)
	at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:217)

17/08/17 05:01:39 INFO mapreduce.Job:  map 100% reduce 19%
17/08/17 05:01:42 INFO mapreduce.Job:  map 100% reduce 20%
17/08/17 05:01:46 INFO mapreduce.Job:  map 100% reduce 21%
17/08/17 05:01:49 INFO mapreduce.Job:  map 100% reduce 22%
17/08/17 05:01:53 INFO mapreduce.Job:  map 100% reduce 24%
17/08/17 05:01:59 INFO mapreduce.Job: Task Id : attempt_1502932926037_0121_r_000002_1, Status : FAILED
Error: org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#2
	at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:153)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:431)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
Caused by: java.lang.OutOfMemoryError: Java heap space
	at org.apache.hadoop.io.BoundedByteArrayOutputStream.<init>(BoundedByteArrayOutputStream.java:56)
	at org.apache.hadoop.io.BoundedByteArrayOutputStream.<init>(BoundedByteArrayOutputStream.java:46)
	at org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput.<init>(InMemoryMapOutput.java:64)
	at org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.unconditionalReserve(MergeManagerImpl.java:304)
	at org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.reserve(MergeManagerImpl.java:294)
	at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyMapOutput(Fetcher.java:547)
	at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:369)
	at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:217)

17/08/17 05:02:00 INFO mapreduce.Job:  map 100% reduce 22%
17/08/17 05:02:13 INFO mapreduce.Job:  map 100% reduce 23%
17/08/17 05:02:16 INFO mapreduce.Job:  map 100% reduce 24%
17/08/17 05:02:19 INFO mapreduce.Job:  map 100% reduce 25%
17/08/17 05:02:29 INFO mapreduce.Job:  map 100% reduce 26%
17/08/17 05:02:29 INFO mapreduce.Job: Task Id : attempt_1502932926037_0121_r_000002_2, Status : FAILED
Error: org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#2
	at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:153)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:431)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
Caused by: java.lang.OutOfMemoryError: Java heap space
	at org.apache.hadoop.io.BoundedByteArrayOutputStream.<init>(BoundedByteArrayOutputStream.java:56)
	at org.apache.hadoop.io.BoundedByteArrayOutputStream.<init>(BoundedByteArrayOutputStream.java:46)
	at org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput.<init>(InMemoryMapOutput.java:64)
	at org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.unconditionalReserve(MergeManagerImpl.java:304)
	at org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.reserve(MergeManagerImpl.java:294)
	at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyMapOutput(Fetcher.java:547)
	at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:369)
	at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:217)

17/08/17 05:02:30 INFO mapreduce.Job:  map 100% reduce 22%
17/08/17 05:02:53 INFO mapreduce.Job:  map 100% reduce 23%
17/08/17 05:03:03 INFO mapreduce.Job:  map 100% reduce 24%
17/08/17 05:03:27 INFO mapreduce.Job:  map 100% reduce 25%
17/08/17 05:03:50 INFO mapreduce.Job:  map 100% reduce 26%
17/08/17 05:04:04 INFO mapreduce.Job:  map 100% reduce 100%
17/08/17 05:04:05 INFO mapreduce.Job: Job job_1502932926037_0121 failed with state FAILED due to: Task failed task_1502932926037_0121_r_000002
Job failed as tasks failed. failedMaps:0 failedReduces:1

17/08/17 05:04:06 INFO mapreduce.Job: Counters: 43
	File System Counters
		FILE: Number of bytes read=7137384868
		FILE: Number of bytes written=15511473172
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=5422979652
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=552
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Job Counters 
		Failed reduce tasks=6
		Killed reduce tasks=2
		Launched map tasks=138
		Launched reduce tasks=8
		Data-local map tasks=131
		Rack-local map tasks=7
		Total time spent by all maps in occupied slots (ms)=1478583
		Total time spent by all reduces in occupied slots (ms)=610135
		Total time spent by all map tasks (ms)=1478583
		Total time spent by all reduce tasks (ms)=610135
		Total vcore-seconds taken by all map tasks=1478583
		Total vcore-seconds taken by all reduce tasks=610135
		Total megabyte-seconds taken by all map tasks=1514068992
		Total megabyte-seconds taken by all reduce tasks=624778240
	Map-Reduce Framework
		Map input records=46305141
		Map output records=75980690
		Map output bytes=8205914520
		Map output materialized bytes=8357878384
		Input split bytes=14904
		Combine input records=0
		Spilled Records=140860842
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=56184
		CPU time spent (ms)=1476420
		Physical memory (bytes) snapshot=37159116800
		Virtual memory (bytes) snapshot=125817716736
		Total committed heap usage (bytes)=27465351168
	org.apache.hadoop.examples.WorkGen$Counters
		MAP_BYTES_WRITTEN=7598069000
		MAP_RECORDS_WRITTEN=75980690
	File Input Format Counters 
		Bytes Read=5422964748
	org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl$TaskVerdict
		ORIGINAL_WIN=138
	org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl$Topo
		Fn_nn=138
Exception in thread "main" java.io.IOException: Job failed!
	at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:865)
	at org.apache.hadoop.examples.WorkGen.run(WorkGen.java:279)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
	at org.apache.hadoop.examples.WorkGen.main(WorkGen.java:290)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
